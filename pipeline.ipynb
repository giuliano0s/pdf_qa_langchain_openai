{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model/parser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "#embedding/vector store\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "#system\n",
    "import os\n",
    "\n",
    "OPENAI_KEY = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 new files found, processing and merging\n"
     ]
    }
   ],
   "source": [
    "# Scan folder\n",
    "files = os.listdir('./data/unprocessed_data/')\n",
    "embeddings = OpenAIEmbeddings(openai_api_key = OPENAI_KEY)\n",
    "\n",
    "if len(files)>0:\n",
    "    print(f'{len(files)} new files found, processing and merging')\n",
    "\n",
    "    #Load files\n",
    "    loaders = [PyPDFLoader('./data/unprocessed_data/' + x) for x in files]\n",
    "\n",
    "    docs = []\n",
    "    for loader in loaders:\n",
    "        docs.extend(loader.load())\n",
    "\n",
    "    #Process\n",
    "    llm = ChatOpenAI(openai_api_key = OPENAI_KEY, model_name = 'gpt-3.5-turbo-16k', temperature=0.5)\n",
    "    splits_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"Formate o texto para que fique entendível para usar de contexto em uma LLM. me dê apenas o output como resposta:\"),\n",
    "        (\"user\", \"{input}\")\n",
    "    ])\n",
    "    splits_parser = StrOutputParser()\n",
    "    splits_chain = splits_prompt | llm | splits_parser\n",
    "    for n in range(len(docs)):\n",
    "        docs[n].page_content = splits_chain.invoke({\"input\": f\"{docs[n].page_content}\"})\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=150)\n",
    "    splits = text_splitter.split_documents(docs)\n",
    "\n",
    "    #Vector merging\n",
    "    vector = FAISS.from_documents(splits, embeddings)\n",
    "    old_vector = FAISS.load_local(\"./data/faiss_index\", embeddings, allow_dangerous_deserialization=True)\n",
    "    vector.merge_from(old_vector)\n",
    "\n",
    "    vector.save_local(\"./data/faiss_index\")\n",
    "\n",
    "    for file in files:\n",
    "        os.replace(f\"./data/unprocessed_data/{file}\", f\"./data/processed_data/{file}\")\n",
    "\n",
    "else:\n",
    "    print('0 new files found, loading old vector')\n",
    "    vector = FAISS.load_local(\"./data/faiss_index\", embeddings, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chain creation\n",
    "\n",
    "chat_hist = []\n",
    "\n",
    "llm = ChatOpenAI(openai_api_key = OPENAI_KEY, model_name = 'gpt-3.5-turbo-16k', temperature=0.5)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\"Você deve memorizar os editais do IFRS canoas para responder questões sobre ele.\n",
    "\n",
    "        Cursos de Licenciatura, Bacharel e Tecnólogo são considerados cursos de nível superior.\n",
    "\n",
    "        Dê informações completas.\n",
    "\n",
    "        <context>\n",
    "        {context}\n",
    "        </context>\n",
    "\n",
    "        Historico de mensagens: {history}\n",
    "\n",
    "        Pergunta: {input}\"\"\")\n",
    "\n",
    "#retrieval\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "retriever = vector.as_retriever(search_kwargs={'k': 10, 'score_treshold': 0.9},\n",
    "                                search_type=\"similarity\")\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O valor mensal das bolsas de monitoria está definido da seguinte forma:\n",
      "- Para uma carga horária de 4 horas semanais, o valor é de R$ 175,00/mês.\n",
      "- Para uma carga horária de 8 horas semanais, o valor é de R$ 350,00/mês.\n"
     ]
    }
   ],
   "source": [
    "msg = \"Qual o salario pago pra quem é monitor?\"\n",
    "\n",
    "response = retrieval_chain.invoke({\"input\": msg,\n",
    "                                   \"history\": chat_hist})\n",
    "\n",
    "print(response[\"answer\"])\n",
    "\n",
    "chat_hist.append({'user':msg})\n",
    "chat_hist.append({'agent':response[\"answer\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
